import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn import metrics
import random

from pandas import DataFrame
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from matplotlib.colors import ListedColormap
from mpl_toolkits.mplot3d import Axes3D
# This code will suppress warnings.
import warnings
from sklearn.exceptions import ConvergenceWarning
warnings.simplefilter("ignore", ConvergenceWarning)

#export OMP_NUM_THREADS=1
#export USE_SIMPLE_THREADED_LEVEL3=1


def neural_network(xtrain,ytrain,xtest,ytest,classification,unktest,srrid):
    # Now make plots of Training Accuracy and plots of Testing Accuracy for NN, one layer 
    models=[]
    trainacc = []
    testacc = []
    bestlearnrate=[]
    best_alpha=[]
    noderange = range(1,11,1)
    
    alpharange = np.logspace(-6,0,5)
    learnrate = np.logspace(-3,-1,3)
    
    for nodecnt in noderange:
            
        bestacc = 0
        bestalpha = 0
        bestrate = 0   
        for alpha in alpharange:
            for rate in learnrate:
    
  
    
                clf = MLPClassifier(hidden_layer_sizes=(nodecnt),\
                            activation='relu', solver='sgd', \
                            learning_rate='adaptive', batch_size='auto',\
                            shuffle=True,alpha=alpha, \
                            nesterovs_momentum=True,verbose=False,\
                            momentum=0.9,early_stopping=False,\
                            max_iter=200, learning_rate_init=rate)
        
                clf.fit(xtrain, ytrain)
    
                if clf.score(xtest,ytest)> bestacc:
                    bestacc = clf.score(xtest,ytest)
                    bestalpha = alpha
                    bestrate = rate
                    mod=clf
    
      
     
    
        trainacc.append( mod.score(xtrain,ytrain) )
        testacc.append( mod.score(xtest,ytest) )
        
        bestlearnrate.append(bestrate)
        best_alpha.append(bestalpha)
        models.append(mod)
        
       
    plt.plot(noderange,trainacc,color='black')
    plt.title('Training accuracy as a function of # nodes')
    plt.xlabel('# nodes')
    plt.ylabel('accuracy')
    plt.savefig('TrainingNN.png')
    
       
    plt.plot(noderange,testacc,color='black')
    plt.title('Testing accuracy as a function of # nodes')
    plt.xlabel('# nodes')
    plt.ylabel('accuracy')
    plt.savefig('TestingNN.png')
    
    idx = testacc.index(max(testacc))
  
   
    
    test_acc= models[idx].score(xtest,ytest)
    #Predict unk
    print(test_acc,max(testacc))
    #Predict unk
 
    pred = models[idx].predict(unktest)    
    pred=pd.DataFrame(data=pred.flatten())
    pred.columns =[classification]
    pred.insert(len(pred.columns),"TestAcc",[testacc],True)
    pred.insert(len(pred.columns),"ID",[srrid],True)
    pred.to_csv("/work/LAS/xgu-lab/jahaltom/Ancestry/" + classification+'NeuralNetworkResults',sep='\t',mode='a',index=False,header=None)    
        
        


def RandomForest_Classifier(xtrain,ytrain,xtest,ytest,classification,unktest,srrid):
     
    models=[]
    
    
    numtreerange = [1,5,10,25,50,100,200]
    overalltestacc = []
    overalltrainacc=[]
    bestNumTrees=[]
    depthrange = range(1,11)
    for depth in depthrange:
        
        best_num_trees = 0
        bestTest_acc = 0
        bestTrain_acc=0
       
        for num_trees in numtreerange:
            clf = RandomForestClassifier(bootstrap=True,n_estimators=num_trees,max_features='auto',criterion='gini',random_state=1,max_depth=depth)
            clf.fit(xtrain, ytrain)
           
            testacc= clf.score(xtest,ytest)
            trainacc=clf.score(xtrain,ytrain)
            if testacc >= bestTest_acc:
                bestTest_acc = testacc
                best_num_trees = num_trees
                bestTrain_acc=trainacc
                mod=clf



        models.append(mod)
        overalltrainacc.append( bestTrain_acc )
        overalltestacc.append(bestTest_acc) 
        bestNumTrees.append(best_num_trees)
        
    idx = overalltestacc.index(max(overalltestacc))
    #clf = RandomForestClassifier(bootstrap=True,n_estimators=bestNumTrees[idx],max_features=None,criterion='gini',max_depth=idx+1)
    #models[idx].fit(xtrain, ytrain)   
            
    testacc= models[idx].score(xtest,ytest)
    
    #Predict unk
    print(testacc,max(overalltestacc))
    
    
    pred = models[idx].predict(unktest)    
    pred=pd.DataFrame(data=pred.flatten())
    pred.columns =[classification]
    pred.insert(len(pred.columns),"TestAcc",[testacc],True)
    pred.insert(len(pred.columns),"ID",[srrid],True)
    pred.to_csv("/work/LAS/xgu-lab/jahaltom/Ancestry/" + classification+'RandomForestResults',sep='\t',mode='a',index=False,header=None)     
        
   
        
   


    plt.plot(depthrange,overalltrainacc,color='black')
    plt.title('Training accuracy as a function of max_depth')
    plt.xlabel('max_')
    plt.ylabel('accuracy')
    plt.savefig('TrainingRF.png')
    
    
    plt.plot(depthrange,overalltestacc,color='black')
    plt.title('Testing accuracy as a function of max_depth')
    plt.xlabel('max_depth')
    plt.ylabel('accuracy')            
    plt.savefig('TestingRF.png')
    


#SVMs with Polynomial Kernels
def svm(xtrain,ytrain,xtest,ytest,classification,unktest,srrid):
    models=[]
    degree = 1
    max_iter=1000
    
    Cvals = np.logspace(-4,2,25,base=10)
    gamma_vals = [1] #np.logspace(-2,2,25,base=10)
    
    
    bestg=[]
    bestC=[]
    bestTest=[]
    bestTrain=[]
    for degree in [1,2,3,4]:
        bestc = 0
        bestgamma = 0
        besttrainacc = 0
        besttestacc = 0
        
        
   
        for c in Cvals:
            for gamma in gamma_vals:
    
                clf = SVC(C=c, kernel='poly', degree=degree, gamma=gamma, coef0=1.0, shrinking=True, probability=False,max_iter=max_iter)
    
                clf.fit(xtrain, ytrain)
    
                trainacc = clf.score(xtrain, ytrain)
                testacc = clf.score(xtest, ytest)
    

    
                if testacc > besttestacc:
                    besttestacc = testacc
                    besttrainacc = trainacc
                    bestgamma = gamma
                    bestc = c
                    mod=clf
        models.append(mod)          
        bestg.append(bestgamma)
        bestC.append(bestc)
        bestTest.append(besttestacc)
    
    idx = bestTest.index(max(bestTest))    
    

    
   
    testacc= models[idx].score(xtest,ytest)
    print(testacc,max(bestTest))
    
    #Predict unk
 
    pred = models[idx].predict(unktest)    
    pred=pd.DataFrame(data=pred.flatten())
    pred.columns =[classification]
    pred.insert(len(pred.columns),"TestAcc",[testacc],True)
    pred.insert(len(pred.columns),"ID",[srrid],True)
    pred.to_csv("/work/LAS/xgu-lab/jahaltom/Ancestry/" + classification+'SVMResults',sep='\t',mode='a',index=False,header=None) 






    
        # Z = clf.predict(np.c_[x1mesh.ravel(), x2mesh.ravel()])
    
        # # Put the result into a color plot
        # Z = Z.reshape(x1mesh.shape)
    
        # # Plot the training points with the mesh
        # plt.figure()
        # plt.pcolormesh(x1mesh, x2mesh, Z, cmap=cmap_light)
        # ytrain_colors = [y-1 for y in ytrain]
        # plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain_colors, cmap=cmap_bold, s=20)
        # plt.xlim(x1_min, x1_max)
        # plt.ylim(x2_min, x2_max)
        # plt.title('SVM Poly %i -- Training Set'%(degree))
        # plt.xlabel('Feature 1')
        # plt.ylabel('Feature 2')
    
        # # Plot the testing points with the mesh
        # plt.figure()
        # plt.pcolormesh(x1mesh, x2mesh, Z, cmap=cmap_light)
        # ytest_colors = [y-1 for y in ytest]
        # plt.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest_colors, cmap=cmap_bold, s=20)
        # plt.xlim(x1_min, x1_max)
        # plt.ylim(x2_min, x2_max)
        # plt.title('SVM Poly %i -- Testing Set'%(degree))
        # plt.xlabel('Feature 1')
        # plt.ylabel('Feature 2')

   
    
  
    
def PCA(vcf,srrid,metadata_path,run):
        #Import metadata and gather samples for each person in 1KGP."igsr-1000 genomes on grch38.tsv"
        metadata=pd.read_csv(metadata_path,sep='\t')
        samples=metadata['Sample name'].copy()
        
        #Randomly suffle samples
        random.shuffle(samples)
        #Devide samples up into 0.70 train and 0.30 test. 
        x=len(samples)
        tr=x*0.70
        tr = int(round(tr, 0))
        tst=x-tr
        train=samples[0:tr]
        test=samples[tr:len(samples)]
       
        test=test.to_frame()
        test.columns =['Sample name']
      
        #Add unkown to 1KGP samples
        samples=samples.to_frame()
        samples.loc[len(samples.index)] = [srrid]
        #Specify all samples for PCA 
        file = pd.concat([samples, samples,samples], axis=1)
        file.to_csv("samples", sep=" ",header=False,index=False)
        #Specify samples to train PCA with, all other samples will be projected. 
        train=train.to_frame()
        train.to_csv("train", sep=" ",header=False,index=False)
        train.columns =['Sample name']
        
        #Run PLINK PCA
        shell("plink --vcf " + vcf + " --pca 20 --within samples --mac 1 --pca-clusters train --out " + run)
        
        #PCA data from plink
        vardata= pd.read_csv(run + '.eigenvec',delimiter=r"\s+",names=['Drop','Sample name','PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14','PC15','PC16','PC17','PC18','PC19','PC20'])
        vardata=vardata.drop(['Drop'], axis=1)
        
        
  
        
        #Merge sample metadata with PCA data
        data=pd.merge(metadata,vardata,on=['Sample name'])
        
        #Make training and test sets same as PCA.       
        Train = pd.merge(data,train,on=['Sample name'])
        x_train=Train[['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14','PC15','PC16','PC17','PC18','PC19','PC20']]  # Features
        y_train=Train['Superpopulation name']  # Labels
        
        Test = pd.merge(data,test,on=['Sample name'])
        x_test=Test[['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14','PC15','PC16','PC17','PC18','PC19','PC20']]  # Features
        y_test=Test['Superpopulation name']  # Labels
        
        
        
        unk_test = vardata[vardata['Sample name'] == srrid ]
        unk_test=unk_test[['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14','PC15','PC16','PC17','PC18','PC19','PC20']]
      
        
          
        if run == "first":
        
            RandomForest_Classifier(x_train,y_train,x_test,y_test,"SuperPopulation",unk_test,srrid)
            neural_network(x_train,y_train,x_test,y_test,"SuperPopulation",unk_test,srrid)
            svm(x_train,y_train,x_test,y_test,"SuperPopulation",unk_test,srrid)
            
            #Make training and test sets same as PCA.       
            
            y_train=Train['Population name']  # Labels              
            y_test=Test['Population name']  # Labels
            
            
            RandomForest_Classifier(x_train,y_train,x_test,y_test,"SubPopulation",unk_test,srrid)
            neural_network(x_train,y_train,x_test,y_test,"SubPopulation",unk_test,srrid)
            svm(x_train,y_train,x_test,y_test,"SubPopulation",unk_test,srrid)
        else:
            y_train=Train['Population name']  # Label               
            y_test=Test['Population name']  # Labels
            
            RandomForest_Classifier(x_train,y_train,x_test,y_test,"SubPopulation",unk_test,srrid)
            neural_network(x_train,y_train,x_test,y_test,"SubPopulation",unk_test,srrid)
            svm(x_train,y_train,x_test,y_test,"SubPopulation",unk_test,srrid)


#Read in run accession IDs from txt file. 
with open ("RAids.txt") as f:
    ra=f.read().splitlines()
# output directory
DIR='output'




rule all:
	input: ["{wd}/{sample}/SubPopulationNeuralNetworkResults".format(sample=s,wd=DIR) for s in ra]

rule isec:
    input: "{wd}/{sample}/vcf.finished.gz"
    
    output: "{wd}/{sample}/Merged.vcf.gz"
            
    shell:
        """
        cd {wildcards.wd}/{wildcards.sample}
        
        gunzip -c  vcf.finished.gz | grep -v  '\./\.' > final.filtered.vcf
       
       
        bgzip final.filtered.vcf
        tabix -fp vcf final.filtered.vcf.gz

        #By chr 
        
        for i in $(seq 1 22); do echo $i >> list; done
        echo X >> list
            
        cat list | while read i; do
            #all sites with matching positions
            bcftools isec -c all /work/LAS/xgu-lab/jahaltom/Ancestry/data/filtered.ALL.chr$i.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz final.filtered.vcf.gz --output-type z -r $i  -p dir.$i        
            #only sites with all alleles identical.(1/1,0/1). 0/0 will be filtered out along with those alt alleles that dont match between 1KG and unk.
            cd dir.$i
            rm 0000.vcf.gz
            bcftools isec -c none 0002.vcf.gz 0003.vcf.gz --output-type z  -p dir
            cd dir
         
         
            #Remove 0/0(reference) from 0001.vcf.gz(unk unique) then use remaining postions(alt alleles that dont match)  to remove alleles from (0003 and 0002) that were made from "-c all".
            gunzip -c 0001.vcf.gz | grep -v "0/0" | grep -v "#" | awk -F'\t' -v OFS='\t' '{{print $1,$2}}' > List
            
            #If List is not empty, then
            if [ -s "List" ]
            then 
                mv List ../
                cd ../
                bcftools view -T ^List 0002.vcf.gz --output-type z  > 1KG.vcf.gz
                bcftools view -T ^List 0003.vcf.gz --output-type z  > unk.vcf.gz
                bcftools index -t unk.vcf.gz
                bcftools index -t 1KG.vcf.gz
                #Merge unk and 1KG 
                bcftools merge 1KG.vcf.gz unk.vcf.gz  --output-type z  > merged.$i.vcf.gz
            else
                mv List ../
                cd ../
                bcftools index -t 0002.vcf.gz
                bcftools index -t 0003.vcf.gz
                #Merge unk and 1KG 
                bcftools merge 0002.vcf.gz 0003.vcf.gz  --output-type z  > merged.$i.vcf.gz
            fi
            
            bcftools index -t merged.$i.vcf.gz
            mv merged.$i.vcf.gz* ../            
            cd ../
        done   
        rm *final*
        rm -r *dir*
        bcftools concat --output-type z -o Merged.vcf.gz *merged.*
        tabix -fp vcf Merged.vcf.gz
        rm *merged*       
        """


rule PCA:
    input: "{wd}/{sample}/Merged.vcf.gz"
    
    output:
        "{wd}/{sample}/SubPopulationNeuralNetworkResults"
    run:
        srrid=str(output).split("/")[1]
        os.chdir(DIR + "/" + srrid)
        vcf=str(input).split("/")[2]
        metadata_path="/work/LAS/xgu-lab/jahaltom/Ancestry/data/metadata.tsv"
        run="first"
        PCA(vcf,srrid,metadata_path,run)
        
        
        
        
        
        #metadata=metadata[metadata['Superpopulation code'] == "EAS" ]
        #metadata.to_csv('EASmeta.tsv',sep='\t',index=False,header=True)
        #run="second"
        #metadata_path="/ocean/projects/mcb200036p/jahaltom/Ancestry/data/EASmeta.tsv"
       # PCA(vcf,srrid,metadata_path,run)
       # shell("rm first* second*")
        
        




        
        


